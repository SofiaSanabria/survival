---
title: "survival + random forests for churn"
author: "pedro.concejerocerezo@telefonica.com"
date: "10 de noviembre de 2015"
output: html_document
---

## Objectives

I have prepared this post as documentation for a speech I will give on [November 12th] with my colleagues of [Grupo-R madRid](http://madrid.r-es.org/). In our previous meeting [Jesús Herranz](http://www.alimentacion.imdea.org/equipo/investigadores/dr-jesus-herranz-valera) gave us a good [introduction on survival models](http://madrid.r-es.org/wp-content/uploads/2015/10/Ana%CC%81lisis-Supervivencia-R-Madrid-2015_10.pdf), but he reserved all the good stuff for his [workshop on random forests for survival](https://github.com/cjgb/sitio7jr/blob/master/sitio/ponencias/jesus_heranz.pdf), which happened in our recent [VII R-hispano users group congress](http://r-es.org/7jornadasR/) -maybe the best event about R in Spain. 

I had recently prepared an introduction on survival models focused on business settings -as a contribution to the applicability of these wonderful models outside the more frequent biomedical field- as my [first techie blog post -in spanish](https://pedroconcejero.wordpress.com/2015/10/26/analisis-de-supervivencia-para-rotacion-de-clientes/) (!). In the meantime I also found this absolutely gReat tutorials on survival for business by [Dayne Batten](http://daynebatten.com/category/survival-analysis/), and now the idea is to quickly apply part of the lessons learnt at Jesus' workshop to the churn dataset. 

We will actually follow only half of Jesus' applications, in particular the libraries [randomForestSRC](http://www.ccs.miami.edu/~hishwaran/rfsrc.html) -gReatest thanks to [Hemant Ishwaran](http://www.ccs.miami.edu/~hishwaran/ishwaran.html), and [ggRandomForests](https://cran.r-project.org/web/packages/ggRandomForests/index.html) for visualization.

Let's go into the code. It should work on any R environment, but please be very careful with some hints for installing the above libraries. If you find any problem please write a comment!

##Setup your R environment 

Be sure to change your directory:

```{r}
setwd("d:/survival")
```

Load (and install if necessary) the required libraries (but see below special requirement about the randomForestSRC library). 

```{r}

list.of.packages <- c("survival", 
                      "caret",
                      "glmnet",
                      "rms",
                      "doParallel",
                      "risksetROC")

new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

library(survival, quietly = TRUE)
library(caret, quietly = TRUE)
library(glmnet, quietly = TRUE)
library(rms, quietly = TRUE)
library(risksetROC, quietly = TRUE)

library(doParallel, quietly = TRUE)                    
registerDoParallel(detectCores() - 1 )  ## registerDoMC( detectCores()-1 ) in Linux

detectCores()
options(rf.cores = detectCores() - 1, 
        mc.cores = detectCores() - 1)  ## Cores for parallel processing
```

Read more about [doParallel](https://cran.r-project.org/web/packages/doParallel/vignettes/gettingstartedParallel.pdf).

By default, doParallel uses multicore functionality on Unix-like systems and snow functionality on Windows. Note that the multicore functionality only runs tasks on a single computer, not a cluster of computers.

randomForestSRC package allows parallelization but the library binaries are different for Windows and Linux, so you must go to [Hemant Ishwaran's rfsrc page](http://www.ccs.miami.edu/~hishwaran/rfsrc.html) and download the zip file and install it as source.

My system is a windows 7 machine, so I am using one version of this zip. Use the appropriate one for your platform. Please verify it loads in your system. 

```{r}

install.packages("http://www.ccs.miami.edu/~hishwaran/rfsrc/randomForestSRC_1.6.0.zip", 
                 repos = NULL, 
                 type = "source")

library(randomForestSRC)

```

And only after this you can install ggRandomForests, with very useful plotting functions for the random forests objects created with randomForestSRC.

```{r}

install.packages("ggRandomForests", 
                 repos = 'http://cran.us.r-project.org') #since you had source before
library(ggRandomForests)

```

## Load and explore the data about churn

I found these Churn data (artificial based on claims similar to real world) suggested at [stackoverflow question](http://stackoverflow.com/questions/27080207/survival-analysis-for-telecom-churn-using-r). Data are part of [UCI machine learning](http://archive.ics.uci.edu/ml/) training sets, also more quickly found at[http://www.sgi.com/tech/mlc/db/](http://www.sgi.com/tech/mlc/db/).


```{r}
nm <- read.csv("http://www.sgi.com/tech/mlc/db/churn.names", 
               skip = 4, 
               colClasses = c("character", "NULL"), 
               header = FALSE, 
               sep = ":")[[1]]

dat <- read.csv("http://www.sgi.com/tech/mlc/db/churn.data", 
                header = FALSE, 
                col.names = c(nm, "Churn"),
                colClasses = c("factor",
                               "numeric",
                               "factor",
                               "character",
                               rep("factor", 2),
                               rep("numeric", 14),
                               "factor"))
# test data

test <- read.csv("http://www.sgi.com/tech/mlc/db/churn.test", 
                header = FALSE, 
                col.names = c(nm, "Churn"),
                colClasses = c("factor",
                               "numeric",
                               "factor",
                               "character",
                               rep("factor", 2),
                               rep("numeric", 14),
                               "factor"))
```

This is a quick exploration of training dataset. You have 3333 unique customer id's (phone numbers), account.length is the age (time dimension), which seems to be months, but I am not totally sure, and you have 15% drop-outs, which is a quite high churn rate but then consider we have >10 years span.

```{r}
dim(dat)
summary(dat)
length(unique(dat$phone.number))
hist(dat$account.length)
table(dat$Churn)/nrow(dat)*100

```

And about the test set. You have exactly 1667 rows, exactly half of the training set.

```{r}
summary(test);dim(test)
```

## Random Forests for Survival

Random Forests (RF) is a machine learning technique which builds a large number of decision trees that:
- are based on bootstrap samples. Each tree is based on a random sample with replacement of all observations.
- each tree division is based on a random sample of predictors.
- There is no prunning, trees are as long as possible, they are not "cut".

For building each RF tree a part of the observations is not used (37% aprox.). This is called out-of-bag -OOB- sample and is used for a honest estimate of the model predictive capability.

[Random Survival Forest (RSF)](http://www.jstatsoft.org/article/view/v050i11/v50i11.pdf) is a class of survival prediction models, those that use data on the life history of subjects (the response) and their characteristics (the predictor variables). In this case, it extends the RF algorithm for a target which is not a class, or a number, but a survival curve. The library is actually so clever that given a particular target it automatically selects the relevant algorith. There are four families of random forests:
regression forests for continuous responses
classification forests for factor responses
Survival forests for right-censored survival settings
competing risk survival forests for competing risk scenarios

RF is now a standard to effectively analyze a large number of variables, of many different types, with no previous variable selection process. It is not parametric, and in particular for survival target it does not assume the proportional risks assumption.

rfsrc requires all data to be either numeric or factors. So you must filter out character, date or other types of variables. Survival object requires a numeric (0/1) target, and in my R environment I have had problems to input factors into the analysis, so I quickly drop those variables (in previous analyses they were not any relevant). And we quickly -and dirtly- convert into dummies (0/1) two relevant factors and the target (Churn).

```{r}

dat$phone.number <- NULL
dat$state <- NULL
dat$area.code <- NULL

dat$international.plan <- as.numeric(dat$international.plan) - 1
dat$voice.mail.plan <- as.numeric(dat$voice.mail.plan) - 1
dat$Churn <- as.numeric(dat$Churn) - 1

summary(dat)
```

You use rfsrc() to build a RF model with the following parameters:
– formula :  response variable and predictors
– data : data frame containing data
– ntree:  total number of trees
– mtry:  number of variables entering in each division as candidates. By default sqrt(p) in classification and survival and p/3 in regression.
– nodesize : minimum number of observations in a terminal node (survival, 3).
– nsplit : number of random points to explore in continous predictors.
– importance = T : prints out variable importance ranking (if not use importance=“none”).
– proximity = T : to compute this metric.

Handling factors is not that easy. See the "Allowable data types and issues related to factors" part in `[rfsrc documentation](https://cran.r-project.org/web/packages/randomForestSRC/randomForestSRC.pdf). 

Let's try a simple RF model with 50 trees and nsplit 2.

```{r}

out.rsf.1 <- rfsrc(Surv(account.length, Churn) ~ . , 
                   data = dat,
                   ntree = 50, 
                   nsplit = 2)

out.rsf.1

```

Computation is intensive (though we have requeste a very simple model and we do not have too many data). rsfrc() permutesall values of all variables in all trees. However, at least for me, the parallelization implementation works flawlessly and gives output very quickly.

The $importance object contains variables importance, in same order as in input dataset. We sort it out to show a ranking and use ggRandomForestses to plot this object. This library uses ggplot2, so you can easily retouch these plots.

```{r}
imp.rsf.1 <- sort(out.rsf.1$importance, 
                  decreasing = T)
imp.rsf.1

plot(gg_vimp(out.rsf.1))
```

## Predictive ability of RSF

To compute a numeric prediction per case, we sum the risk output estimate along all times. This is equivalent to a risk score, so that higher values correspond to observations with higher observed risk, lower survival. These predictions can be based on all trees or only on the OOB sample.

In RSF, error rate is defined as = 1 – C-index

Let's check C-index

```{r}

length( out.rsf.1$predicted.oob )
head( out.rsf.1$predicted.oob )
sum.chf.oob = apply(out.rsf.1$chf.oob , 1, sum ) 
head(sum.chf.oob )

## OOB Error = 1 – C-index
rcorr.cens(out.rsf.1$predicted.oob, 
           Surv(dat$account.length, dat$Churn))["C Index"]

err.rate.rsf = out.rsf.1$err.rate[ out.rsf.1$ntree ]
err.rate.rsf

## C-index ( Mayor supervivencia se relaciona con menor riesgo )  
rcorr.cens(-out.rsf.1$predicted.oob, 
           Surv(dat$account.length, dat$Churn))["C Index"]
```

## Towards an optimal RFS

An essential parameter is the number of trees. To compute the best number of trees you use importance=“none” so that you do not use unnecessary computing, and set a sufficiently high number of trees. This depends as you could see by default values, on the number of predictor variables. In our example, with just a few variables, a value of a few hundreds looks enough. We then use gg_error() of ggRandomForests to plot the results across the number of trees. You choose the point (minimum number) where plot converges into a minimum. If it does not converge, try with a higher number of trees.

```{r}

out.rsf.3 <- rfsrc( Surv(account.length, Churn) ~ . , 
                   data = dat, 
                   ntree = 200, 
                   importance = "none", 
                   nsplit = 1)
out.rsf.3

plot(gg_error(out.rsf.3))

```

## Predictive ability applied to test data: C Index

Let's do as usual: apply our model to the test data and check for predictive ability. 

First of all, remember to make same mods as we did to the training to test set!!!

```{r}

test$phone.number <- NULL
test$state <- NULL
test$area.code <- NULL

test$international.plan <- as.numeric(test$international.plan) - 1
test$voice.mail.plan <- as.numeric(test$voice.mail.plan) - 1
test$Churn <- as.numeric(test$Churn) - 1

summary(test)

```

We apply the computed model to the test set using, as usual, predict. We then check the C_Index as before.

```{r}

pred.test.fin = predict( out.rsf.3, 
                         newdata = test, 
                         importance = "none" )

rcorr.cens(-pred.test.fin$predicted , 
             Surv(test$account.length, test$Churn))["C Index"]

```

## Predictive ability applied to test data. ROC generalizations

[risksetROC library](https://cran.r-project.org/web/packages/risksetROC/risksetROC.pdf) provides functions to compute the equivalent to a ROC curve and its associated Area Under Curve (AUC) in a time-dependent context. 

This is precisely the greatest advantage: you can compute predictive ability at specific time points or intervals. Let's see predictive ability of OOB samples (in training set) at median time. We must be careful about method as, depending on assumptions, this can be different (see documentation), but let's assume we meet Cox's proportional hazards assumption.

```{r}

w.ROC = risksetROC(Stime = dat$account.length,  
                   status = dat$Churn, 
                   marker = out.rsf.3$predicted.oob, 
                   predict.time = median(dat$account.length), 
                   method = "Cox", 
                   main = paste("OOB Survival ROC Curve at t=", 
                                median(dat$account.length)), 
                   lwd = 3, 
                   col = "red" )
                          
w.ROC$AUC

```

For risksetROC to compute AUC along an interval you use risksetAUC using tmax (maximum time). You get a very nice plot of AUC across time. This is still OOB samples.

```{r}

w.ROC = risksetAUC(Stime = dat$account.length,  
                   status = dat$Churn, 
                   marker = out.rsf.3$predicted.oob,
                   tmax = 250)

```

Let's do the same for test data.

```{r}

w.ROC = risksetAUC(Stime = test$account.length,  
                   status = test$Churn, 
                   marker = pred.test.fin$predicted, 
                   tmax = 220, 
                   method = "Cox")

```

And with a plot, at good local maximum prediction time, 190.

```{r}

w.ROC = risksetROC(Stime = test$account.length,  
                   status = test$Churn, 
                   marker = pred.test.fin$predicted, 
                   predict.time = 190, 
                   method = "Cox", 
                   main = paste("OOB Survival ROC Curve at t=190"), 
                   lwd = 3, 
                   col = "red" )
 
w.ROC$AUC

```

And with a plot, at maybe best prediction time, 220.

```{r}

w.ROC = risksetROC(Stime = test$account.length,  
                   status = test$Churn, 
                   marker = pred.test.fin$predicted, 
                   predict.time = 220, 
                   method = "Cox", 
                   main = paste("OOB Survival ROC Curve at t=220"), 
                   lwd = 3, 
                   col = "red" )
 
w.ROC$AUC

```

## Conclusions

- randomForestSRC is a fabulous library for all tasks related to Random Forests computing, and it has a wondeRful implementation for survival targets.
- I have found some issues for handling factors that I shall explore, so that you can input factors directly into the algorithm (without converting into dummies).
- RSF gives important advantages over traditional (classification) RF when you have survival (time dependent) targets. In particular you can check predictive capability and its changes across time, and many other useful results for obtaining knowledge of your analysis. All this maintaing the predictive ability of the traditional classification models.

I hope this encourages you to try these models and these libraries for your analyses. Feel free to comment your results, or problems if you found them!
